{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we import all the libraries required to make our code work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer \n",
    "import os\n",
    "from collections import Counter\n",
    "import heapq\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV, splitting all the rows to tsv\n",
    "\n",
    "As required in the homework, we created .tsv documents for every row of the Airbnb DataFrame. We used the standard function writer but we had to include the encoding parameter as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counter = 0\n",
    "with open('Airbnb_Texas_Rentals.csv', 'r',encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        if row: # if row is not empty, write a file with this row\n",
    "            filename = \"doc/doc_%s.tsv\" % str(counter)\n",
    "            with open(filename, 'r',encoding=\"utf8\") as csvfile_out:\n",
    "                writer = csv.writer(csvfile_out,delimiter=\"\\t\")\n",
    "                writer.writerow(row[1:])\n",
    "                counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "\n",
    "The following task of our homework wanted us to clean the data contained in the tsv files. FIrst of all, to get the total number of documents in the folder we used listdir, a function of the os package. Then we defined functions to go through all the docs in the folder to scan and clean them. We removed punctuation, stemmed words and we lowered case every character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DIR = '/Users/canta/Desktop/HMW3/doc/'\n",
    "ndocs= (len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))\n",
    "\n",
    "words=[]\n",
    "def cleandata(doc):\n",
    "    txt = doc.read().split('\\t')    \n",
    "    description = txt[4]\n",
    "    title = txt[7]\n",
    "    description = description + \" \" + title    \n",
    "    #we replace the new lines \"tags\"\n",
    "    description = description.replace('\\\\r', ' ')\n",
    "    description = description.replace('\\\\n', ' ')    \n",
    "    #tokenize the document, strips out the punctuation\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens_d = tokenizer.tokenize(description)\n",
    "    #lower case of every word\n",
    "    tokens_d = [w.lower() for w in tokens_d]   \n",
    "    #stemming\n",
    "    ps = PorterStemmer()\n",
    "    tokens_d = [ps.stem(w) for w in tokens_d]   \n",
    "    #remove stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    tokens_d = [w for w in tokens_d if not w in stop]   \n",
    "    #remove duplicates\n",
    "    tokens_d = set(tokens_d)\n",
    "    #append all words we found in words list\n",
    "    for word in tokens_d:\n",
    "        words.append(word)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e5a4e35bf85f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmyfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/canta/Desktop/HMW3/doc/doc_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcleandata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-64a63ff5997f>\u001b[0m in \u001b[0;36mcleandata\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtokens_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens_d\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#remove stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mtokens_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens_d\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#remove duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \"\"\"\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[0;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No such file or directory: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range (1,ndocs): #LATER WILL BECOME NDOCS \n",
    "\n",
    "    myfile = open('/Users/canta/Desktop/HMW3/doc/doc_'+str(i)+'.tsv', 'r',encoding=\"utf8\")\n",
    "    cleandata(myfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vocabulary - a dictionary with words as keys and integers as IDs\n",
    "\n",
    "vocabulary = {}\n",
    "count = 1\n",
    "for word in words:\n",
    "    vocabulary[word] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#INVERTED INDEX and docs dictionary: a nested dictionary to retrieve the frequencies for every word in each doc\n",
    "\n",
    "invIndex = {}\n",
    "docs = {}\n",
    "\n",
    "for i in range (1,ndocs): \n",
    "\n",
    "    myfile = open('/Users/canta/Desktop/HMW3/doc/doc_'+str(i)+'.tsv', 'r',encoding=\"utf8\")\n",
    "    filename = os.path.basename((myfile.name).replace('.tsv', ''))\n",
    "    txt = myfile.read().split('\\t')    \n",
    "    #very common code with the first part - we should build a function in some way\n",
    "    description = txt[4]\n",
    "    title = txt[7]\n",
    "    description = description + \" \" + title    \n",
    "    description = description.replace('\\\\r', ' ')\n",
    "    description = description.replace('\\\\n', ' ')\n",
    "      \n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens_inv = tokenizer.tokenize(description)\n",
    "    \n",
    "    \n",
    "    #computed here, the length takes into account the stopwords and excludes the punctuation\n",
    "    nparole = len(tokens_inv) #compute here to count ...?\n",
    "    \n",
    "    \n",
    "    #lower case \n",
    "    tokens_inv = [w.lower() for w in tokens_inv]\n",
    "   \n",
    "    #stemming\n",
    "    ps = PorterStemmer()\n",
    "    tokens_inv = [ps.stem(w) for w in tokens_inv]\n",
    "    \n",
    "    #stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    tokens_inv = [w for w in tokens_inv if not w in stop]\n",
    "    \n",
    "    #the counter allows us to create a dict with every word as a key and the number of times it appears in the doc\n",
    "    # as value\n",
    "    c = Counter(tokens_inv)\n",
    "    \n",
    "    c = dict(c)\n",
    "    #to compute easily the tf for each doc we put into the dictionary a key containing the lenght\n",
    "    c['LUNGHEZZA2018'] =  nparole\n",
    "    docs[filename] = c\n",
    "    \n",
    "    tokens_inv = set(tokens_inv)\n",
    "  \n",
    "    #mechanism to build the inverted index dict\n",
    "    for word in tokens_inv:\n",
    "        \n",
    "        \n",
    "        #for every word: if it is the first time it appears, create a new key in the dict and append the name of\n",
    "        # the document in which it is present. if it's already in the dict, append the name of the doc\n",
    "        if(word in vocabulary.keys()):\n",
    "            \n",
    "            if (vocabulary[word]) in invIndex:\n",
    "                invIndex[vocabulary[word]].append(filename)\n",
    "            else:\n",
    "                invIndex[vocabulary[word]] = [filename]\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the query as input\n",
    "query = str(input())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the word IDs of the query\n",
    "\n",
    "#manipulate the text of the query: stemming, lower case, strip punctuation and stopwords, remove duplicates\n",
    "\n",
    "query = query.split()\n",
    "ps = PorterStemmer()\n",
    "query = [ps.stem(w) for w in query]\n",
    "query = [w.lower() for w in query]\n",
    "stop = set(stopwords.words('english'))\n",
    "query = [w for w in query if not w in stop]\n",
    "query = set(query)\n",
    "\n",
    "#look at vocabulary dict: from the key of the dict (word) we get the IDs\n",
    "ids = []\n",
    "for word in query:\n",
    "    if word in vocabulary.keys():\n",
    "        ids.append(vocabulary[word])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have the IDs, now let's get the list of documents in which they're in\n",
    "querydict = {}\n",
    "for id in ids:\n",
    "    querydict[id] = invIndex[id]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need only the docs containing all the words, so the point here is to make the intersection between all the lists\n",
    "#of docs\n",
    "\n",
    "querylist = []\n",
    "\n",
    "for key in querydict.keys():\n",
    "    querylist.append(querydict[key])\n",
    "\n",
    "#intersection\n",
    "result = set(querylist[0]).intersection(*querylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanres(result):\n",
    "    goodres=[]\n",
    "    urllist=[]\n",
    "    for each in result:\n",
    "                myfiles=open('/Users/canta/Desktop/HMW3/doc/'+str(each)+'.tsv','r', encoding=\"utf8\")\n",
    "                txt = myfiles.read().split('\\t')                \n",
    "                if txt[7] not in urllist: \n",
    "                    goodres.append(each)\n",
    "                    urllist.append(txt[7])\n",
    "                \n",
    "    return goodres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=cleanres(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the results of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHOWING THE RESULTS OF THE QUERY\n",
    "def showres(result):\n",
    "    title=[]\n",
    "    descr=[]\n",
    "    city=[]\n",
    "    url=[]\n",
    "    for each in result:\n",
    "        myfiles=open('/Users/canta/Desktop/HMW3/doc/'+str(each)+'.tsv','r', encoding=\"utf8\")\n",
    "        txt = myfiles.read().split('\\t')\n",
    "        title.append(txt[7])\n",
    "        descr.append(txt[4])\n",
    "        city.append(txt[2])\n",
    "        url.append(txt[8])\n",
    "\n",
    "    data_tuples = list(zip(title,descr,city,url))\n",
    "    table=pd.DataFrame(data_tuples,columns = [\"Title\", \"Description\", \"City\", \"Link\"])\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=showres(result)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3.2 - tfIdf Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW Inverted Index, includes tfIdf scores\n",
    "\n",
    "import math\n",
    "\n",
    "tfidfIdx = {}\n",
    "\n",
    "for key in invIndex: #here key is the wordID - so: for every word\n",
    "    \n",
    "    idf_den = len(invIndex[key]) #n° of different docs containing the word\n",
    "    \n",
    "    realWord = (list(vocabulary.keys())[list(vocabulary.values()).index(key)]) #getting word from its ID\n",
    "    \n",
    "    for each in invIndex[key]: #each is the doc_i, for the word, take all documents in which it appears\n",
    "        \n",
    "        #accessing docs dict by two keys: name of doc and word. get the count and divide it by doc length\n",
    "        tf = docs[each][realWord]/(docs[each] ['LUNGHEZZA2018']) \n",
    "        tfidf = tf * math.log10(ndocs/idf_den)\n",
    "        \n",
    "        if(key in tfidfIdx.keys()):\n",
    "            tfidfIdx[key].append((each, tfidf))\n",
    "        else:\n",
    "            tfidfIdx[key] = [(each, tfidf)]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortcuts to compute the value inside the function below\n",
    "\n",
    "def eucl_norm(a):\n",
    "    return math.sqrt(np.dot(a, a))\n",
    "\n",
    "def cosine_similarity(a, b, c):\n",
    "    return np.dot(a,b) / (eucl_norm(c) * eucl_norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(doc):\n",
    "    DOCUMENTO = str(doc)\n",
    "    myfile = open('/Users/canta/Desktop/HMW3/doc/'+str(doc)+'.tsv', 'r',encoding=\"utf8\")\n",
    "    txt = myfile.read().split('\\t')\n",
    "    description = txt[4]\n",
    "    title = txt[7]\n",
    "    description = description + \" \" + title\n",
    "    description = description.replace('\\\\r', ' ')\n",
    "    description = description.replace('\\\\n', ' ')\n",
    "    tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens_d =tokenizer.tokenize(description)\n",
    "    tokens_d = [w.lower() for w in tokens_d]\n",
    "    ps = PorterStemmer()\n",
    "    tokens_d = [ps.stem(w) for w in tokens_d]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    tokens_d = [w for w in tokens_d if not w in stop]\n",
    "    tokens_d = set(tokens_d)\n",
    "    \n",
    "    #entire document\n",
    "    docwordId =[] #all word IDs of document\n",
    "\n",
    "    for word in tokens_d: #word in tokens_d is the actual word in the doc\n",
    "        docwordId.append(vocabulary[word]) #we look into vocabulary to get the IDs   \n",
    "    \n",
    "    #we are getting tfidf scores from tfidfIdx for every word id in the doc    \n",
    "    docsQuery = []\n",
    "    scores = [] #vector of tfidf scores for all words in the doc:   ||d||\n",
    "    for idx in docwordId: #for every ID - always all words in doc\n",
    "        for each in tfidfIdx[idx]: #give me its score, given that we're analyzing one document per time\n",
    "            if(each[0] == DOCUMENTO):\n",
    "                scores.append(each[1]) #each[1] is the score of that word in the doc\n",
    "                if(idx in ids):\n",
    "                    docsQuery.append(each[1])\n",
    "                \n",
    "    #query tfidf  |q| and ||q||\n",
    "    querytfidf = []\n",
    "    for ID in ids:\n",
    "        idf_den = len(invIndex[ID])\n",
    "        tf = 1/len(ids)\n",
    "        idf = math.log10(ndocs/idf_den)\n",
    "        querytfidf.append(tf*idf)\n",
    "    \n",
    "    \n",
    "    a=cosine_similarity(docsQuery,querytfidf, scores)\n",
    "    return (a,doc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result:\n",
    "    print(cosine(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heap Data Structure\n",
    "\n",
    "According to the homework requests, we used an [heap data structure](https://www.geeksforgeeks.org/heap-data-structure/) to handle the documents resulting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = []\n",
    "K=10  #LENGTH WE HAVE DECIDED \n",
    "heapq.heapify(H)\n",
    "for each in result:\n",
    "    if len(H)<K:\n",
    "        heapq.heappush(H,cosine(each))\n",
    "    else:\n",
    "        a,b=cosine(each)\n",
    "        heapq.heapreplace(H,cosine(each))\n",
    "# Use heapify to rearrange the elements\n",
    "docli=[]\n",
    "punt=[]\n",
    "heapq.heapify(H) \n",
    "for each in H:\n",
    "    x,y=each\n",
    "    docli.append(y)\n",
    "    punt.append(x)\n",
    "    \n",
    "   \n",
    "\n",
    "table=showres(docli)\n",
    "table['Scores']=punt\n",
    "table.sort_values(\"Scores\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE A NEW SCORING FUNCTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined three new functions: one for the distance index, one for the price index and one for final new scoring function(plus one nice graph to show the distribution for the prices per bedrooms). \n",
    "\n",
    "## definedistance\n",
    "\n",
    "So, the starting point is our set **result**. This set contains all the documents in which the query is contained. We decide to compute the distance of every documents from the city centre of its own city centre. We have done that usign the Geopymodule. We have also normalized our values by dividing by the maxium distance found in one of the documents of result. Here is the formula:\n",
    "\n",
    "$$\\frac{distance of the single document}{maximum distance of one document in result}$$\n",
    "\n",
    "The outuput of this formula is a list with all the distances normalized.\n",
    "\n",
    "## defprice\n",
    "\n",
    "Using the same starting point(result=set containing all the documents in which the query is contained), we decided to use the variables 'average_rate_per_night' and 'bed_room_count' to calculate our final scoring function. First of all, we computed the 'price per bedroom' variable with:\n",
    "\n",
    "$$listpricesperbed=\\frac{averageratepernight}{bedroomcount}$$\n",
    "\n",
    "we assigned 'bed_room_count'==0 if its values was 'Studio'. At this point we decided to use the quantile distribution of this new variable we have created and we opted to boost all the documents appearing in the second quartile(Practically speaking, the ones that were below the median(low price per bedroom). We usand this kind of weighting process:\n",
    "\n",
    " -listpricesperbed[doc]*0.25 if it was contained in the **first quartile**\n",
    " \n",
    " -listpricesperbed[doc]*0.40 if it was contained in the **second quartile**\n",
    "\n",
    " -listpricesperbed[doc]*0.25 if it was contained in the **third quartile**\n",
    "\n",
    " -listpricesperbed[doc]*0.10 if it was contained in the **fourth quartile**\n",
    " \n",
    " Obviously the sum of the weights equals to 1. And finally, we divided each element by the maximum contained in that list.\n",
    " \n",
    " This fucntion returns a list of prices per bedrooms with all the weighting process applied to it. \n",
    " \n",
    " ## def scoringfunction\n",
    " \n",
    " As our last function we created the new final scoring function. This takes in every document in result with its distance and its price(given by the previous functions). We used this formula to obtain our final scoring function:\n",
    " \n",
    " $$newscore==(Weightforprice)*(1-pricesperbed)+(weightfordistance)(1-distance)$$\n",
    " \n",
    " THe weights for both the distance and the price can be decided arbitrarily and we decided to weight more the distance than the prices. The correspondant weights are: \n",
    " $$Weightforprice==0.40$$\n",
    " \n",
    " $$Weightforprice==0.60$$\n",
    " \n",
    " This function the returns a value to rank our documents with and the name of the documents which refers to.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " **P.S.**: We know we could have written and called everything in a single function but we decided to split the funcitons in order to make everything more understandable (Hopefully we did succeed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definedistance(result):\n",
    "    distances=[]\n",
    "    for each in result:\n",
    "        myfiles=open('/Users/canta/Desktop/HMW3/doc/'+str(each)+'.tsv','r', encoding=\"utf8\")\n",
    "        doc = myfiles.read().split(\"\\t\")\n",
    "        city =doc[2].lower()\n",
    "        long=doc[6]\n",
    "        lat=doc[5]\n",
    "        geolocator = Nominatim(user_agent=\"mb\") #bo\n",
    "        locationQ = ((geolocator.geocode(city)).latitude, (geolocator.geocode(city)).longitude) # we should put the city of the query here\n",
    "        loc = (lat,long) #coordinates retrieved by the doc, pretty easy - to be substituted with above\n",
    "        distance = geodesic(locationQ, loc).miles\n",
    "        distances.append(distance)\n",
    "    mass=max(distances)\n",
    "    finaldist = [x / mass for x in distances]\n",
    "    return (list(finaldist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defprice(result):\n",
    "    listpricesperbed =[]\n",
    "    for each in result:\n",
    "        myfiles=open('/Users/canta/Desktop/HMW3/doc/'+str(each)+'.tsv','r', encoding=\"utf8\")\n",
    "        doc = myfiles.read().split(\"\\t\")\n",
    "        price=doc[0]\n",
    "        bedrooms=(doc[1])\n",
    "        price=price.replace(\"$\",\"\")\n",
    "        if(bedrooms=='Studio'):\n",
    "            listpricesperbed.append(0)\n",
    "            continue          \n",
    "        listpricesperbed.append(int(price)/int(bedrooms))\n",
    "    global finallist\n",
    "    finallist=np.array(listpricesperbed)\n",
    "    firstp=np.percentile(finallist,25)\n",
    "    secondp=np.percentile(finallist,50)\n",
    "    thirdp=np.percentile(finallist,75)\n",
    "    fourthp=np.percentile(finallist,100)\n",
    "    for i in range(len(finallist)):\n",
    "        if finallist[i]<firstp:\n",
    "            finallist[i]=finallist[i]*0.25\n",
    "        elif finallist[i]<=secondp and finallist[i]>firstp :\n",
    "            finallist[i]=finallist[i]*0.40\n",
    "        elif finallist[i]<=thirdp and finallist[i]>secondp :\n",
    "            finallist[i]=finallist[i]*0.25\n",
    "        elif finallist[i]<=fourthp and finallist[i]>thirdp:\n",
    "            finallist[i]=finallist[i]*0.10 \n",
    "    lista=finallist/(max(finallist))\n",
    "    return(list(lista))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoringfunction(doc,distanza,price):\n",
    "    newscore=(0.40*(1-price))+(0.60*(1-distanza))\n",
    "    return (newscore,doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heap Data Structure\n",
    "\n",
    "According to the homework requests, we used an [heap data structure](https://www.geeksforgeeks.org/heap-data-structure/) to handle the documents resulting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distan=definedistance(result)  \n",
    "pricesperbeds=defprice(result)\n",
    "H = []\n",
    "K=10  #LENGTH WE HAVE DECIDED \n",
    "heapq.heapify(H)\n",
    "i=0\n",
    "for each in result:\n",
    "    if len(H)<K:\n",
    "        heapq.heappush(H,scoringfunction(each,distan[i],pricesperbeds[i]))\n",
    "    else:\n",
    "        heapq.heapreplace(H,scoringfunction(each,distan[i],pricesperbeds[i]))\n",
    "    i+=1\n",
    "# Use heapify to rearrange the elements\n",
    "docli=[]\n",
    "punt=[]\n",
    "heapq.heapify(H) \n",
    "for each in H:\n",
    "    x,y=each\n",
    "    docli.append(y)\n",
    "    punt.append(x)\n",
    "    \n",
    "   \n",
    "\n",
    "table=showres(docli)\n",
    "table['New Scores']=punt\n",
    "table.sort_values(\"New Scores\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.kdeplot(finallist, shade=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=finallist )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func():\n",
    "    \n",
    "    geolocator = Nominatim(user_agent=\"Texas\")\n",
    "    \n",
    "    text = input(\"Please enter either the 'place' or the 'coordinates' of your choice [place/coordinates]: \")\n",
    "    \n",
    "    \n",
    "    \n",
    "    if text == \"place\":\n",
    "        place = input(\"Type the name of the place you are searching for: \")\n",
    "        rad = int(input(\"Enter the distance in km: \"))\n",
    "        location = geolocator.geocode(place) \n",
    "        #This gives the coordinates of the place\n",
    "        loc_input = (location.latitude, location.longitude)\n",
    "    elif text == \"coordinates\":\n",
    "        lat = float(input(\"Enter the latitude: \"))\n",
    "        long = float(input(\"Enter the longitute: \"))\n",
    "        loc_input = (lat, long)\n",
    "        #This gives the address of the coordinates\n",
    "        location = geolocator.reverse(loc_input) \n",
    "        print(\"The coordinates you entered belongs to this address: \" + location.address) \n",
    "\n",
    "        rad = int(input(\"Enter the distance in km: \"))\n",
    "        \n",
    "    else:\n",
    "        return print(\"Incorrect input! Enter only 'place' or 'coordinates'\") \n",
    "    \n",
    "    tooltip = \"Click here\"\n",
    "    #Creating main map\n",
    "    main_map = folium.Map( \n",
    "    location = loc_input,\n",
    "    zoom_start = 12\n",
    "    )\n",
    "    rad = rad*1000 \n",
    "    #creating s circle within the prescribed distance\n",
    "    folium.Circle( \n",
    "    location = loc_input, \n",
    "    radius = rad,\n",
    "    color = '#3186cc',\n",
    "    fill = True,\n",
    "    fill_color = '#3186cc'\n",
    "    #fill_opacity: 0.6\n",
    "    ).add_to(main_map)\n",
    "\n",
    "    #Addning markers\n",
    "    folium.Marker(location = loc_input, icon = folium.Icon(color='red')).add_to(main_map) \n",
    "    \n",
    "    #Create clusters of the points\n",
    "    mark_cluster = MarkerCluster().add_to(main_map)\n",
    "    \n",
    "    #Creating markers for each location in dataframe\n",
    "    for i, x in info.iterrows():\n",
    "        point = (x[\"latitude\"], x[\"longitude\"])\n",
    "        geography = geodesic(point, loc_input).meters\n",
    "        \n",
    "        \n",
    "        if geography <= rad:\n",
    "            folium.Marker(location = (x[\"latitude\"], x[\"longitude\"]),\n",
    "                          popup = folium.Popup('<a href=\"'+x[\"url\"]+'\"'+'target=\"_blank\"> [Click the url for more information] </a>'),\n",
    "                          tooltip = tooltip\n",
    "                         ).add_to(mark_cluster)\n",
    "    \n",
    "    return main_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
